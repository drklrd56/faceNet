{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "faceNet_project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qBRhgJj1BBM",
        "colab_type": "text"
      },
      "source": [
        "# Mount and Extract "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3YPbSjBYZ1l",
        "colab_type": "code",
        "outputId": "0ad4e392-c10f-4a50-e7b2-1fa7a6fb70c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JXsLPD7UgRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip  \"/content/drive/My Drive/Colab Notebooks/lfw.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "cc2f4fa6-84ca-4f5d-a324-fecad88c0d9a",
        "_cell_guid": "9b0e5da7-7054-4ce6-b95f-4c36ccaaa731",
        "trusted": true,
        "id": "ENooxiQHT5_U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import random\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader,Dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Zt9ywjc1Kmt",
        "colab_type": "text"
      },
      "source": [
        "# FaceNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L0UtPxr-T5_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class faceNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(faceNet, self).__init__()\n",
        "        #conv1\n",
        "        self.layer1 = nn.Sequential(\n",
        "            \n",
        "            #inputSize = 220x220x3 outputSize= 110x110x64\n",
        "            nn.Conv2d(in_channels= 3, out_channels= 64, kernel_size= 7, stride= 2, padding= 3),\n",
        "            #inputSize = 110x110x64 outputSize= 55x55x64\n",
        "            nn.MaxPool2d(kernel_size= 3, stride= 2, padding= 1),\n",
        "            nn.BatchNorm2d(num_features= 64, eps= 0.00001), #assume rnorm is batchNorm\n",
        "        )\n",
        "        #conv2\n",
        "        self.layer2 = nn.Sequential(    \n",
        "            #inputSize = 55x55x64 outputSize= 55x55x64\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 64, kernel_size= 1, stride= 1, padding= 0),\n",
        "            #inputSize = 55x55x64 outputSize= 55x55x192\n",
        "            nn.Conv2d(in_channels= 64, out_channels= 192, kernel_size= 3, stride= 1, padding= 1),\n",
        "            nn.BatchNorm2d(num_features= 192, eps= 0.00001), #assume rnorm is batchNorm\n",
        "            #inputSize = 55x55x192 outputSize= 28x28x192\n",
        "            nn.MaxPool2d(kernel_size= 3, stride= 2, padding= 1),\n",
        "        )\n",
        "        #conv3\n",
        "        self.layer3 = nn.Sequential(    \n",
        "            #inputSize = 28x28x192 outputSize= 28x28x192\n",
        "            nn.Conv2d(in_channels= 192, out_channels= 192, kernel_size= 1, stride= 1, padding= 0),\n",
        "            #inputSize = 28x28x192 outputSize= 28x28x384\n",
        "            nn.Conv2d(in_channels= 192, out_channels= 384, kernel_size= 3, stride= 1, padding= 1),\n",
        "            #inputSize = 28x28x384 outputSize= 14x14x384\n",
        "            nn.MaxPool2d(kernel_size= 3, stride= 2, padding= 1),\n",
        "        )\n",
        "        #conv4\n",
        "        self.layer4 = nn.Sequential(    \n",
        "            #inputSize = 14x14x384 outputSize= 14x14x384\n",
        "            nn.Conv2d(in_channels= 384, out_channels= 384, kernel_size= 1, stride= 1, padding= 0),\n",
        "            #inputSize = 14x14x384 outputSize= 14x14x256\n",
        "            nn.Conv2d(in_channels= 384, out_channels= 256, kernel_size= 3, stride= 1, padding= 1),\n",
        "        )\n",
        "        #conv5\n",
        "        self.layer5 = nn.Sequential(    \n",
        "            #inputSize = 14x14x256 outputSize= 14x14x256\n",
        "            nn.Conv2d(in_channels= 256, out_channels= 256, kernel_size= 1, stride= 1, padding= 0),\n",
        "            #inputSize = 14x14x256 outputSize= 14x14x256\n",
        "            nn.Conv2d(in_channels= 256, out_channels= 256, kernel_size= 3, stride= 1, padding= 1),\n",
        "        ) \n",
        "        #conv6\n",
        "        self.layer6 = nn.Sequential(        \n",
        "            #inputSize = 14x14x256 outputSize= 14x14x256\n",
        "            nn.Conv2d(in_channels= 256, out_channels= 256, kernel_size= 1, stride= 1, padding= 0),\n",
        "            #inputSize = 14x14x256 outputSize= 14x14x256\n",
        "            nn.Conv2d(in_channels= 256, out_channels= 256, kernel_size= 3, stride= 1, padding= 1),\n",
        "            #inputSize = 14x14x256 outputSize= 7x7x256\n",
        "            nn.MaxPool2d(kernel_size= 3, stride= 2, padding= 1),\n",
        "        )\n",
        "        self.FClayer = nn.Sequential(    \n",
        "            #inputSize = 7x7x256 outputSize= 12544\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(7*7*256,32*128),\n",
        "            nn.Linear(32*128,32*128),\n",
        "            nn.Linear(32*128,128),\n",
        "        )\n",
        "    def forward(self,X):\n",
        "        Y = self.layer1(X)\n",
        "        Y = self.layer2(Y)\n",
        "        Y = self.layer3(Y)\n",
        "        Y = self.layer4(Y)\n",
        "        Y = self.layer5(Y)\n",
        "        Y = self.layer6(Y)\n",
        "        Y = self.FClayer(Y)\n",
        "        Y = f.normalize(Y)\n",
        "        return Y\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4eXPmuk1RwF",
        "colab_type": "text"
      },
      "source": [
        "# Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oiIfZClBT6AJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def triplet_loss(anchor, positive, negative, margin= 0.2):\n",
        "    \"\"\"Build the triplet loss over a batch of embeddings.\n",
        "    For each anchor, we get the hardest positive and hardest negative to form a triplet.\n",
        "    Args:\n",
        "        anchor,positive,negative: tensor of shape [batch_size, 128]\n",
        "        margin: margin for triplet loss\n",
        "    Returns:\n",
        "        loss: scalar tensor containing the triplet loss\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the (encoding) distance between the anchor and the positive\n",
        "    distance_positive = (anchor - positive).pow(2).sum(1)\n",
        "    \n",
        "    # Compute the (encoding) distance between the anchor and the negative\n",
        "    distance_negative = (anchor - negative).pow(2).sum(1)\n",
        "    \n",
        "    # subtract the two previous distances and add alpha.\n",
        "    losses = f.relu(distance_positive - distance_negative + margin)\n",
        "\n",
        "    return losses.mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "2382af4e-25e1-4b04-a8a3-c00947094502",
        "_cell_guid": "5ca6c27f-1de3-4c62-a922-1f5ab73ab664",
        "trusted": true,
        "id": "NOjnKLbeT6Ai",
        "colab_type": "code",
        "outputId": "1aa4b9b1-7a7d-4138-e2ca-ec28ffb759fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = faceNet()\n",
        "model = model.float()\n",
        "model = model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yomg4B-L1WTz",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLlqDhPtuyW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = \"/content/lfw-deepfunneled/lfw-deepfunneled/\"\n",
        "csv_dir = \"/content/lfw_allnames.csv\"  # name , imageCount(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw5H56g2ok1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Triplet(Dataset):\n",
        "    def __init__(self, train_set, transform = None):\n",
        "        self.transform = transform\n",
        "        temp = train_set.groupby(['name']).image_path.apply(lambda x: list(x.values)).reset_index()\n",
        "        self._generate_triplets(temp['name'],temp.index, temp['image_path'])\n",
        "        del temp\n",
        "    \n",
        "    def _generate_triplets(self,names, labels, image_paths): #Ids, classes:\n",
        "        self.triplets = []\n",
        "        num_labels = len(labels)\n",
        "        for i in range(num_labels):\n",
        "            #select anchor image and positive image\n",
        "            \n",
        "            # select a label at random to be the positive class\n",
        "            positive_label = np.random.choice(labels) \n",
        "            while len(image_paths[positive_label]) < 2:\n",
        "                # repeat until a class with more than 1 image is found\n",
        "                positive_label = np.random.choice(labels) \n",
        "            \n",
        "            #select a random image to be the anchor from the selected class\n",
        "            anchor = np.random.randint(0, len(image_paths[positive_label]))\n",
        "            #select a random image to be the positive from the selected class\n",
        "            postive = np.random.randint(0, len(image_paths[positive_label]))\n",
        "            # make sure that anchor and positive are the same image\n",
        "            while anchor == postive:\n",
        "                postive = np.random.randint(0, len(image_paths[positive_label]))\n",
        "\n",
        "            #selecting negative image\n",
        "            \n",
        "            # select a label at random to be the negative class\n",
        "            negative_label = np.random.choice(labels)\n",
        "            # make sure that negative image label is not the same as anchor,positive\n",
        "            while positive_label == negative_label:\n",
        "                negative_label = np.random.choice(labels)\n",
        "                \n",
        "            negative = np.random.randint(0, len(image_paths[negative_label]))\n",
        "            \n",
        "            self.triplets.append([image_paths[positive_label][anchor], image_paths[positive_label][postive],\n",
        "                                  image_paths[negative_label][negative], names[positive_label], names[negative_label]])\n",
        "    \n",
        "    def read_image(self, imgName, img_dim = 220):\n",
        "        path = root_dir + str(imgName)\n",
        "        im = Image.open(path)\n",
        "        width, height = im.size   # Get dimensions\n",
        "        left = (width - img_dim)/2\n",
        "        top = (height - img_dim)/2\n",
        "        right = (width + img_dim)/2\n",
        "        bottom = (height + img_dim)/2\n",
        "        im = im.crop((left, top, right, bottom))\n",
        "        return np.array(im)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplets)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        anchor, positive, negative, pos_name, neg_name = self.triplets[idx]\n",
        "        if self.transform:\n",
        "            anchor = self.transform(self.read_image(anchor))\n",
        "            positive = self.transform(self.read_image(positive))\n",
        "            negative = self.transform(self.read_image(negative))\n",
        "            \n",
        "        sample = {'anchor': anchor, 'positive': positive, 'negative': negative,\n",
        "                  'positive_name': pos_name, 'negative_name': neg_name}\n",
        "   \n",
        "        return sample\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYSkvXokveZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LFWDataset(Dataset):\n",
        "    def __init__(self, transform= None, train= True):\n",
        "        self.train = train\n",
        "        self.transform = transform\n",
        "        self.train_set,self.test_set = self.train_test_split()\n",
        "        \n",
        "    def train_test_split(self):\n",
        "        \n",
        "        # copied from kaggle\n",
        "        train_set = pd.read_csv(csv_dir)\n",
        "        train_set = train_set.loc[train_set.index.repeat(train_set['images'])]\n",
        "        train_set['image_path'] = 1 + train_set.groupby('name').cumcount()\n",
        "        train_set['image_path'] = train_set.image_path.apply(lambda x: '{0:0>4}'.format(x))\n",
        "        train_set['image_path'] = train_set.name + \"/\" + train_set.name + \"_\" + train_set.image_path + \".jpg\"\n",
        "        # end \n",
        "        \n",
        "        temp = train_set.where(train_set['images'] >= 2).dropna()\n",
        "        random_class = random.sample(range(temp.index[0], temp.index[-1]), 10)\n",
        "        test_set = pd.DataFrame(columns= train_set.columns)\n",
        "        for i in random_class:\n",
        "          index = temp.index[i]\n",
        "          x = temp[temp.index == index]\n",
        "          x = x.dropna()\n",
        "          test_set = test_set.append(x, ignore_index = True)\n",
        "          del x\n",
        "        del temp\n",
        "        train_set = pd.merge(train_set,test_set, indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1)\n",
        "        train_set = train_set.drop(['images'], axis= 1).reset_index().drop(['index'], axis= 1) \n",
        "        test_set = test_set.drop(['images'], axis= 1).reset_index().drop(['index'], axis= 1) \n",
        "        return train_set, test_set\n",
        "    \n",
        "    def read_image(self, imgName, img_dim = 220):\n",
        "        path = root_dir + str(imgName)\n",
        "        im = Image.open(path)\n",
        "        width, height = im.size   # Get dimensions\n",
        "        left = (width - img_dim)/2\n",
        "        top = (height - img_dim)/2\n",
        "        right = (width + img_dim)/2\n",
        "        bottom = (height + img_dim)/2\n",
        "        im = im.crop((left, top, right, bottom))\n",
        "        return np.array(im)\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.train == True:\n",
        "            return len(self.train_set)\n",
        "        else:\n",
        "            return len(self.test_set)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if self.train == True:\n",
        "          image = self.read_image(self.train_set.iloc[idx]['image_path'])\n",
        "          if self.transform:\n",
        "              image = self.transform(image)\n",
        "            \n",
        "          sample = {'image': image, 'name': self.train_set.iloc[idx]['name']}\n",
        "\n",
        "        else:\n",
        "            image = self.read_image(self.test_set.iloc[idx]['image_path'])\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            \n",
        "            sample = {'image': image, 'name': self.test_set.iloc[idx]['name']}\n",
        "        return sample\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_uDIBs8AT6AX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_transform = transforms.Compose([\n",
        "            transforms.ToPILImage(),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5])])\n",
        "dataset = LFWDataset(transform = image_transform, train= True, )\n",
        "triplets = Triplet(dataset.train_set, transform= image_transform )\n",
        "data_loader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=1)\n",
        "triplet_loader = DataLoader(triplets, batch_size=64, shuffle=True, num_workers=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByGsAOFF1fQV",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "791847ae-8157-4c51-91b5-f21dc7b3a41c",
        "_cell_guid": "dd6280ef-5869-46a9-b8aa-d8c7ae1ce923",
        "trusted": true,
        "id": "_5c0m2-fT6Au",
        "colab_type": "code",
        "outputId": "f049e988-ade6-4d01-d120-491b9a4e962c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "dataset.train = 0 \n",
        "model = model.train()\n",
        "torch.cuda.empty_cache()\n",
        "loss_arr = []\n",
        "idx = 0 \n",
        "total_loss = 0\n",
        "for epoch in range(10):  \n",
        "    for i, batch in enumerate(triplet_loader):\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "        anchor, positive, negative = batch['anchor'],batch['positive'],batch['negative']\n",
        "        anchor, positive, negative = anchor.to(device), positive.to(device), negative.to(device)\n",
        "        anchor, positive, negative = model(anchor.float()), model(positive.float()), model(negative.float())\n",
        "        loss = triplet_loss(anchor, positive, negative, margin= 0.2)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if(idx%100 == 0):\n",
        "            loss_arr.append([idx,loss])\n",
        "        total_loss += loss\n",
        "        idx += 1\n",
        "        del anchor\n",
        "        del positive\n",
        "        del negative\n",
        "    torch.cuda.empty_cache()\n",
        "    print('Epoch:{}, Loss:{}'.format(epoch+1, loss))\n",
        "print('Finished Training')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:10, Loss:0.003505938919261098\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwFFIGeo1jhL",
        "colab_type": "text"
      },
      "source": [
        "# Loss Plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wepUrSS3T6BW",
        "colab_type": "code",
        "outputId": "f157737b-512c-4ea7-bc3c-9ce7ade1f692",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "X = np.array([i for i,j in loss_arr])\n",
        "Y = np.array([j.cpu().detach().numpy() for i,j in loss_arr])\n",
        "plt.plot(X,Y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc34d9d14a8>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtfP1eym1m0a",
        "colab_type": "text"
      },
      "source": [
        "# Test train-set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmFeihUL2pjD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten(data, labels, images = None):\n",
        "  data_flatten = []\n",
        "  labels_flatten = []\n",
        "  for i in range(len(data)): #total  batches\n",
        "      for j in range(len(labels[i])): #batch size\n",
        "          data_flatten.append(data[i][j])\n",
        "          labels_flatten.append(labels[i][j])\n",
        "\n",
        "  return data_flatten,labels_flatten"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "L_IVZnLXT6BC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/ADAM2faceNet.pth\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oi6PjRlT6BA",
        "colab_type": "text"
      },
      "source": [
        "**Please restart the notebook after saving the model and then load the model if any GPU memory problems occurs from this point**\n",
        "\n",
        "* please restart the session and run from here \n",
        "* run all blocks uptil model training loop skip torch.save and FileLink blocks also\n",
        "* then upload the faceNet.pth file,if u closed the tab or restarted the session\n",
        "* start execution from here after uploading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCtylDx_E1LI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4662773-1121-4584-eb51-470e08335b8f"
      },
      "source": [
        "#model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/ADAM2faceNet.pth\"))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKZJeZgr10WD",
        "colab_type": "text"
      },
      "source": [
        "# Model Training using KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0G7QkhX8T6BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model.eval()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "083t8iwaT6Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset.train = False # shift to train mode\n",
        "test_encodings = []\n",
        "test_labels = []\n",
        "torch.cuda.empty_cache()\n",
        "for i,batch in enumerate(data_loader):\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.no_grad()\n",
        "    image, label = batch['image'], batch['name']\n",
        "    image = image.to(device)\n",
        "    encoding = model(image.float()) \n",
        "    test_encodings.append(encoding.cpu().detach().numpy())\n",
        "    test_labels.append(label)\n",
        "    del encoding\n",
        "    del label\n",
        "    del image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4S1k03SSXsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce994e3e-296b-4e7c-ab68-ae0a039ef147"
      },
      "source": [
        "print(dataset.train_set.shape, dataset.test_set.shape)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12346, 2) (887, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn9muv4bFNTT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encodings,labels = flatten(test_encodings,test_labels)\n",
        "del test_encodings\n",
        "del test_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIk12dYssvue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_set = pd.DataFrame(list(zip(labels, encodings)), columns= [\"Label\",\"Encoding\"]) \n",
        "test_set = test_set.groupby(['Label']).Encoding.apply(lambda x: list(x.values)).reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oY-K5iqntVXe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_test_split(test_set, split = 0.5):\n",
        "  Xtrain = []; Xtest = []; Ytrain = []; Ytest = []\n",
        "\n",
        "  for i in range(test_set.shape[0]):\n",
        "    encodings = test_set.iloc[i]['Encoding'] # image loc train  >=  image\n",
        "    label = test_set.iloc[i]['Label']\n",
        "    enc_len = len(encodings)\n",
        "    for j in range(enc_len):\n",
        "      if j <= int (enc_len * 0.5):\n",
        "        Xtrain.append(encodings[j])\n",
        "        Ytrain.append(label)\n",
        "      else:\n",
        "        Xtest.append(encodings[j])\n",
        "        Ytest.append(label)\n",
        "    \n",
        "  return np.array(Xtrain),np.array(Xtest),np.array(Ytrain),np.array(Ytest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "K0TOkzp_T6Bg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances \n",
        "import scipy.stats as ss\n",
        "\n",
        "class KNearestNeighbor:\n",
        "    ''' Implements the KNearest Neigbours For Classification... ''' \n",
        "    def __init__(self, k, scalefeatures=False):\n",
        "        self.K=k\n",
        "\n",
        "    def train(self, X, Y):\n",
        "        nexamples,nfeatures=X.shape\n",
        "        self.X_train=X\n",
        "        self.Y_train=Y\n",
        "\n",
        "    def predict(self, X):\n",
        "        num_test = X.shape[0]\n",
        "        y_pred = np.zeros(self.K, dtype = self.Y_train.dtype)\n",
        "        pclass=[]\n",
        "        compute_distance = euclidean_distances(X, self.X_train)\n",
        "  \n",
        "        for x in range(num_test):\n",
        "            SortedDist=np.sort(compute_distance[x])\n",
        "            for y in range(self.K):\n",
        "                index=np.where(SortedDist[y] == compute_distance[x])\n",
        "                y_pred[y]=self.Y_train[index][0]\n",
        "            pclass.append(ss.mode(y_pred)[0][0])\n",
        "        return np.array(pclass)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_OnPs87vUzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Xtrain,Xtest,Ytrain,Ytest = train_test_split(test_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2F0x5f6AzVJh",
        "colab_type": "code",
        "outputId": "9399cadf-86ba-4a3b-a2d3-d625ba3ed73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(Xtrain.shape,Ytrain.shape,Xtest.shape,Ytest.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(451, 128) (451,) (436, 128) (436,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKPYQZUNYdTM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "KNN = KNearestNeighbor(k=3)\n",
        "KNN.train(Xtrain, Ytrain)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOrsMbBlaFNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_train,pred_test = KNN.predict(Xtrain), KNN.predict(Xtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfvpGUybfyLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets find the accuracy on the test set.. \n",
        "total= Ytrain.shape[0]\n",
        "\n",
        "correct_train = incorrect_train = 0 \n",
        "correct_test = incorrect_test = 0\n",
        "\n",
        "for i in range(len(Ytest)):\n",
        "  if pred_test[i] == Ytest[i]:\n",
        "    correct_test += 1\n",
        "  else:\n",
        "    incorrect_test +=1\n",
        "\n",
        "for i in range(len(Ytrain)):\n",
        "  if pred_train[i] == Ytrain[i]:\n",
        "    correct_train += 1\n",
        "  else:\n",
        "    incorrect_train += 1\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIyv2TbAYxV7",
        "colab_type": "code",
        "outputId": "0dbbb0fe-6580-46dc-ca16-4d90eb681bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "\n",
        "print('Test Set Accuracy :')\n",
        "print(\"For Xtrain : \")\n",
        "print('Total correct : ',correct_train, 'Out of : ',len(Ytrain))\n",
        "print('Accuracy:', (correct_train/Ytrain.shape[0] ) * 100,\"%\")\n",
        "print(\"For Xtest : \")\n",
        "print('Total correct : ',correct_test, 'Out of : ',len(Ytest))\n",
        "print('Accuracy:', (correct_test/Ytest.shape[0] ) * 100,\"%\")"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Set Accuracy :\n",
            "For Xtrain : \n",
            "Total correct :  398 Out of :  451\n",
            "Accuracy: 88.24833702882484 %\n",
            "For Xtest : \n",
            "Total correct :  352 Out of :  436\n",
            "Accuracy: 80.73394495412845 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}